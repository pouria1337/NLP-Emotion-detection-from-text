{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get a& filter nrc words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')\n",
    "\n",
    "tf_anger = pd.read_csv('/content/drive/MyDrive/tf-idf/Proper/tfList/tf_anger')\n",
    "tf_disgust = pd.read_csv('/content/drive/MyDrive/tf-idf/Proper/tfList/tf_disgust')\n",
    "tf_fear  = pd.read_csv('/content/drive/MyDrive/tf-idf/Proper/tfList/tf_fear')\n",
    "tf_joy = pd.read_csv('/content/drive/MyDrive/tf-idf/Proper/tfList/tf_joy')\n",
    "tf_sadness = pd.read_csv('/content/drive/MyDrive/tf-idf/Proper/tfList/tf_sadness')\n",
    "\n",
    "len(tf_anger), len(tf_joy)\n",
    "\n",
    "def corr(df):\n",
    "    temp = 0\n",
    "    temp2  = []\n",
    "    #I've deleted 'away', and 'no'\n",
    "    newList = ['you', 'we', 'they', 'them', 'us', 'him', 'her', 'me', 'the', 'an', \"'s\", 'it', 'he', 'she', 'said', 'ah',\n",
    "           'but', 'what', 'where', 'who', 'why', 'when', 'then', 'shall', 'and', 'that',\n",
    "           'there', 'this', 'these', 'those', 'am', 'is', 'was', 'were', 'mr', 'Mr', 'thou', 'oh']\n",
    "    for i in range(len(df['index'])):\n",
    "        if len(df['index'][i]) == 1:\n",
    "            temp+=1\n",
    "            temp2.append(i)\n",
    "        if df['index'][i] in newList:\n",
    "            temp2.append(i)\n",
    "            temp+=1\n",
    "    corrdf = df.drop(temp2, axis=0)\n",
    "    corrdf.reset_index(inplace=True)\n",
    "    corrdf.drop(axis=1, labels=['level_0'], inplace=True)\n",
    "    return corrdf\n",
    "\n",
    "tff_anger = corr(tf_anger)\n",
    "tff_disgust = corr(tf_disgust)\n",
    "tff_fear = corr(tf_fear)\n",
    "tff_joy = corr(tf_joy)\n",
    "tff_sadness = corr(tf_sadness)\n",
    "\n",
    "nrc_df = pd.read_csv('/content/drive/My Drive/NRC.txt', encoding= 'unicode-escape', sep=\"\\t\")\n",
    "nrc_df.columns = ['Word', 'Emotion', 'Member']\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "def getWord(emotion):\n",
    "\n",
    "  not_index = nrc_df[nrc_df['Emotion'] != emotion].index\n",
    "  a = nrc_df.drop(not_index)\n",
    "  a_1 = a.drop(a[a['Member'] != 1].index)\n",
    "  myDF = shuffle(a_1, random_state=33)\n",
    "  myDF = myDF.reset_index().drop(columns='index')\n",
    "  return myDF\n",
    "\n",
    "nrc_anger = getWord('anger')\n",
    "#nrc_anger\n",
    "\n",
    "nrc_disgust = getWord('disgust')\n",
    "nrc_fear = getWord('fear')\n",
    "nrc_joy = getWord('joy')\n",
    "nrc_sadness = getWord('sadness')\n",
    "\n",
    "def HiLex(nrc, tf):\n",
    "    temp = []\n",
    "    tf = list(tf['index'])\n",
    "    for w in nrc['Word']:\n",
    "        if w in tf[:1024]:\n",
    "            temp.append(w)\n",
    "    return temp\n",
    "\n",
    "a = HiLex(nrc_anger, tff_anger)\n",
    "#a\n",
    "\n",
    "d = HiLex(nrc_disgust, tff_disgust)\n",
    "f = HiLex(nrc_fear, tff_fear)\n",
    "j = HiLex(nrc_joy, tff_joy)\n",
    "s = HiLex(nrc_sadness, tff_sadness)\n",
    "\n",
    "l1 = [a,d,f,j,s]\n",
    "addup = []\n",
    "for i in l1:\n",
    "    print(len(i))\n",
    "    addup.append(len(i))\n",
    "\n",
    "\n",
    "added=0\n",
    "for num in addup:\n",
    "    added+=num\n",
    "added/len(addup)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save words to txt files for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2 = []\n",
    "for i in l1:\n",
    "    l2.append(i)\n",
    "\n",
    "len(l2)\n",
    "\n",
    "','.join(l2[0])\n",
    "\n",
    "filetosave = open('wordlist.txt', 'w')\n",
    "for i in l2:\n",
    "    filetosave.write(\",\".join(i))\n",
    "    filetosave.write(\"\\n\")\n",
    "filetosave.close()\n",
    "\n",
    "filetosave2 = open('wordlist2.txt', 'w')\n",
    "for i in l2:\n",
    "    filetosave2.write(\",\".join(i))\n",
    "    filetosave2.write(\"\\n\\n\")\n",
    "filetosave2.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Sim with model:\n",
    "## all-mpnet-base-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_alpha = pd.read_csv('/content/drive/MyDrive/ISEAR_5428/isear_2.csv')\n",
    "\n",
    "import torch\n",
    "!pip install sentence-transformers\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "model = SentenceTransformer('all-mpnet-base-v2')\n",
    "\n",
    "## test QA\n",
    "\n",
    "query_embedding = model.encode(\"London's population\")\n",
    "passage_embedding = model.encode(['London has 9,787,426 inhabitants at the 2011 census',\n",
    "                                  'London is known for its finacial district',\n",
    "                                 'Hello, Tehran is the capital of Iran!',\n",
    "                                  'Britain is a very large kingdom.',\n",
    "                                  'London is a big metropolice'])\n",
    "\n",
    "x = util.cos_sim(query_embedding, passage_embedding)\n",
    "x.shape\n",
    "print(\"Similarity:\", x)\n",
    "\n",
    "## Modeling\n",
    "\n",
    "#Modeling words & converting to tensor\n",
    "#Modeling words\n",
    "passage_passage_anger = model.encode(a)\n",
    "passage_passage_disgust = model.encode(d)\n",
    "passage_passage_fear = model.encode(f)\n",
    "passage_passage_joy = model.encode(j)\n",
    "passage_passage_sadness = model.encode(s)\n",
    "\n",
    "#transforming numpy.array to torch tensor /START\n",
    "passage_passage_anger_tch = torch.from_numpy(passage_passage_anger)\n",
    "passage_passage_disgust_tch = torch.from_numpy(passage_passage_disgust)\n",
    "passage_passage_fear_tch = torch.from_numpy(passage_passage_fear)\n",
    "passage_passage_joy_tch = torch.from_numpy(passage_passage_joy)\n",
    "passage_passage_sadness_tch = torch.from_numpy(passage_passage_sadness)\n",
    "#transforming numpy.array to torch tensor /END\n",
    "\n",
    "passage_list = [passage_passage_anger_tch, passage_passage_disgust_tch, passage_passage_fear_tch, passage_passage_joy_tch, passage_passage_sadness_tch]\n",
    "\n",
    "matrix_01 = []\n",
    "for memory in data_alpha['SIT']:\n",
    "  #query_embedded = model.encode(memory)\n",
    "  query_embedded_tch = torch.from_numpy(model.encode(memory))\n",
    "  for emoWord in passage_list:\n",
    "    matrix_01.append(torch.mean(util.cos_sim(query_embedded_tch, emoWord)))\n",
    "\n",
    "matrix_numpy = []\n",
    "matrix_01_copy = matrix_01.copy()\n",
    "for tensor in matrix_01_copy:\n",
    "  matrix_numpy.append(tensor.numpy())\n",
    "\n",
    "#Changing the matrix dtype to numpy\n",
    "matrix_numpy = np.array(matrix_numpy)\n",
    "type(matrix_numpy)\n",
    "#Converting 7577*1 to 1*7577\n",
    "matrix_numpy_1d = np.ravel(matrix_numpy)\n",
    "\n",
    "#reshaping 1*7577 to 7577*5\n",
    "matrix_2d_alpha = matrix_numpy_1d.reshape((5428,5))\n",
    "matrix_2d_alpha\n",
    "\n",
    "matrix_df = pd.DataFrame(matrix_2d_alpha, columns=['anger', 'disgust', 'fear', 'joy', 'sadness'])\n",
    "matrix_df['Recorded Emotion'] = data_alpha['Field1']\n",
    "#matrix_df.to_csv('/content/drive/MyDrive/tf-idf/mpnet-v2/ss1024(corrected).csv', index=False)\n",
    "matrix_df\n",
    "\n",
    "## Read the SS matrix\n",
    "\n",
    "\n",
    "!pip install --upgrade pandas\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')\n",
    "\n",
    "import pandas as pd\n",
    "matrix_df = pd.read_csv('/content/drive/MyDrive/tf-idf/mpnet-v2/ss1024(corrected).csv')\n",
    "matrix_df\n",
    "\n",
    "## Do ML\n",
    "\n",
    "import numpy as np\n",
    "matrix_b = matrix_df.copy()\n",
    "#Run Random Forest\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "features = list(matrix_b.columns)[:-1]\n",
    "labels = list.pop(list(matrix_b.columns))\n",
    "\n",
    "#splitting features and lables\n",
    "X = matrix_b[features]\n",
    "y = matrix_b[labels]\n",
    "\n",
    "X_np = np.array(X)\n",
    "y_np = np.array(y).ravel()\n",
    "\n",
    "#Creating train/test splits\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_np, y_np, test_size=0.2, random_state=42)\n",
    "len(X_train), len(X_test), len(y_train), len(y_test)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import cross_validate, cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "#function to get metrics from resulting dictionary\n",
    "#and round them to 3 \n",
    "def getScores(dict):\n",
    "  scores = ['test_accuracy', 'test_f1_macro', 'test_precision_macro', 'test_recall_macro']\n",
    "  temp_ls= []\n",
    "  for score in scores:\n",
    "    temp = 0\n",
    "    temp = round(dict[score].mean(), 3)\n",
    "    temp_ls.append(temp)\n",
    "  return temp_ls\n",
    "\n",
    "clf_LoReg = LogisticRegression(random_state=33).fit(X,y)\n",
    "clf_LoReg.score(X,y)\n",
    "LR_cv = cross_validate(clf_LoReg, X_np, y_np, cv=10, scoring=['accuracy', 'precision_macro', 'recall_macro', 'f1_macro'])\n",
    "w = getScores(LR_cv)\n",
    "w\n",
    "\n",
    "## Random Forest\n",
    "\n",
    "#Training RF\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import cross_validate, cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "rf_clf = RandomForestClassifier(n_estimators=100)\n",
    "rf_cv_macro = cross_validate(rf_clf, X_np, y_np, cv=10,scoring=['accuracy', 'precision_macro', 'recall_macro', 'f1_macro'])\n",
    "\n",
    "x = getScores(rf_cv_macro)\n",
    "x\n",
    "\n",
    "## SVM\n",
    "\n",
    "from sklearn import svm\n",
    "svm_clf = svm.SVC(kernel='rbf', C=1, random_state=42)\n",
    "svm_cv = cross_validate(svm_clf, X_np, y_np, cv=10, scoring=['accuracy', 'precision_macro', 'recall_macro', 'f1_macro'])\n",
    "y = getScores(svm_cv)\n",
    "y\n",
    "\n",
    "#fitting SVM\n",
    "svm_clf.fit(X_np, y_np)\n",
    "\n",
    "#save SVM\n",
    "import pickle\n",
    "filename = 'finalized_model2.sav'\n",
    "pickle.dump(svm_clf, open(filename, 'wb'))\n",
    "\n",
    "## G-Boosted\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gb_cv = cross_validate((GradientBoostingClassifier(\n",
    "                        n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0)),X_np, y_np, cv=10,\n",
    "                        scoring=['accuracy', 'precision_macro', 'recall_macro', 'f1_macro'])\n",
    "z = getScores(gb_cv)\n",
    "z\n",
    "\n",
    "## DF for all metrics\n",
    "\n",
    "df_1 = pd.DataFrame(index=['Accuracy','F1-score','Precision', 'Recall'], columns=['Random_Forest', 'SVM', 'G-Boosted_Tree'])\n",
    "df_1['Random_Forest'] = x\n",
    "df_1['SVM'] = y\n",
    "df_1['G-Boosted_Tree'] = z\n",
    "df_1\n",
    "\n",
    "## PerClass\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import cross_validate, cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "rf_clf = RandomForestClassifier(n_estimators=100)\n",
    "from sklearn import svm\n",
    "svm_clf = svm.SVC(kernel='rbf', C=1, random_state=42)\n",
    "\n",
    "matrix_b = matrix_df.copy()\n",
    "#Run Random Forest\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "features = list(matrix_b.columns)[:-1]\n",
    "labels = list.pop(list(matrix_b.columns))\n",
    "\n",
    "#splitting features and lables\n",
    "X = matrix_b[features]\n",
    "y = matrix_b[labels]\n",
    "\n",
    "X_np = np.array(X)\n",
    "y_np = np.array(y).ravel()\n",
    "\n",
    "#Creating train/test splits\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_np, y_np, test_size=0.2, random_state=42)\n",
    "len(X_train), len(X_test), len(y_train), len(y_test)\n",
    "\n",
    "#create df\n",
    "iterables = [[\"anger\", \"disgust\", \"fear\", \"joy\", \"sadness\"], [\"accuracy\", \"f1-score\", \"precision\", \"recall\"]]\n",
    "\n",
    "index = pd.MultiIndex.from_product(iterables, names=[\"Emotion\", \"Metrics\"])\n",
    "\n",
    "df_2 = pd.DataFrame(index = index, columns=['(Random_Forest)', '(SVM)', '(G-Boosted_Tree)', '(NaiveBayes_GaussianNB)'])\n",
    "\n",
    "y_pred_gb = cross_val_predict(GradientBoostingClassifier(n_estimators=100, learning_rate=0.9, max_depth=1, random_state=0), X_np, y_np, cv=10)\n",
    "gb_class_report = classification_report(y_np, y_pred_gb, output_dict=True)\n",
    "\n",
    "y_pred_nbg = cross_val_predict(GaussianNB(), X_np, y_np, cv=10)\n",
    "nbg_class_report = classification_report(y_np, y_pred_nbg, output_dict=True)\n",
    "\n",
    "y_pred_rf = cross_val_predict(rf_clf, X_np, y_np, cv=10)\n",
    "rf_class_report = classification_report(y_np, y_pred_rf, output_dict=True)\n",
    "\n",
    "y_pred_svm = cross_val_predict(svm_clf, X_np, y_np, cv=10)\n",
    "svm_class_report = classification_report(y_np, y_pred_svm,output_dict=True)\n",
    "\n",
    "list1 = ['anger', 'disgust', 'fear', 'joy', 'sadness']\n",
    "list2 = ['f1-score', 'precision', 'recall']\n",
    "\n",
    "#FOR RANDOM--FOREST\n",
    "for i in list1:\n",
    "  for j in list2:\n",
    "    df_2['(Random_Forest)'][i][j] = round(rf_class_report[i][j], ndigits=2)\n",
    "    df_2['(Random_Forest)'][i]['accuracy'] = round(rf_class_report['accuracy'], ndigits=2)\n",
    "\n",
    "#FOR SVM\n",
    "for i in list1:\n",
    "  for j in list2:\n",
    "    df_2['(SVM)'][i][j] = round(svm_class_report[i][j], ndigits=2)\n",
    "    df_2['(SVM)'][i]['accuracy'] = round(svm_class_report['accuracy'], ndigits=2)\n",
    "\n",
    "\n",
    "#(G-Boosted_Tree)\n",
    "for i in list1:\n",
    "  for j in list2:\n",
    "    df_2['(G-Boosted_Tree)'][i][j] = round(gb_class_report[i][j], ndigits=2)\n",
    "    df_2['(G-Boosted_Tree)'][i]['accuracy'] = round(gb_class_report['accuracy'], ndigits=2)\n",
    "\n",
    "\n",
    "#(NaiveBayes_GaussianNB)\n",
    "for i in list1:\n",
    "  for j in list2:\n",
    "    df_2['(NaiveBayes_GaussianNB)'][i][j] = round(nbg_class_report[i][j], ndigits=2)\n",
    "    df_2['(NaiveBayes_GaussianNB)'][i]['accuracy'] = round(nbg_class_report['accuracy'], ndigits=2)\n",
    "\n",
    "df_2\n",
    "\n",
    "## Ranking (Maximum)\n",
    "\n",
    "import numpy as np\n",
    "list2 = [2,4,2,3,10]\n",
    "index_min = np.argmax(list2)\n",
    "index_min\n",
    "\n",
    "list1 = matrix_df.columns\n",
    "#treat each row as a list \n",
    "matrix_df.iloc[4]\n",
    "\n",
    "temp = []\n",
    "for i in range(len(matrix_df.iloc[4])):\n",
    "    x = matrix_df.iloc[4][i]\n",
    "    if type(x) != str:\n",
    "        temp.append(x)\n",
    "temp\n",
    "\n",
    "np.argmax(temp)\n",
    "\n",
    "rank_max = []\n",
    "temp = []\n",
    "for i in range(len(matrix_df)):\n",
    "    temp = []\n",
    "    for j in range(len(matrix_df.iloc[i])):\n",
    "        x = matrix_df.iloc[i][j]\n",
    "        if type(x) != str:\n",
    "            temp.append(x)\n",
    "    y = np.argmax(temp)\n",
    "    rank_max.append(y)\n",
    "#rank_max\n",
    "\n",
    "len(rank_max)\n",
    "\n",
    "labels = matrix_df['Recorded Emotion']\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(labels)\n",
    "le.classes_\n",
    "\n",
    "true_y = le.transform(labels)\n",
    "true_y\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "y_true = true_y\n",
    "y_pred = rank_max\n",
    "#precision_recall_fscore_support(y_true, y_pred, average=None, labels=[0,1,2,3,4])\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "target_names = ['class 0', 'class 1', 'class 2', 'class 3', 'class 4']\n",
    "print(classification_report(y_true, y_pred, target_names=target_names))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "matrix = confusion_matrix(y_true, y_pred, labels=[0,1,2,3,4])\n",
    "matrix.diagonal()/matrix.sum(axis=1)\n",
    "\n",
    "matrix\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "target_names = ['class 0', 'class 1', 'class 2', 'class 3', 'class 4', 'class 5']\n",
    "\n",
    "#Get the confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "#array([[1, 0, 0],\n",
    "#   [1, 0, 0],\n",
    "#   [0, 1, 2]])\n",
    "\n",
    "#Now the normalize the diagonal entries\n",
    "cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "#array([[1.        , 0.        , 0.        ],\n",
    "#      [1.        , 0.        , 0.        ],\n",
    "#      [0.        , 0.33333333, 0.66666667]])\n",
    "\n",
    "#The diagonal entries are the accuracies of each class\n",
    "cm.diagonal()\n",
    "#array([1.        , 0.        , 0.66666667])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
